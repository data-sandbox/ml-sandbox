{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Sentiment Analysis\n",
    "\n",
    "This project aims to experiment with machine learning text classification models for [Amazon reviews](https://huggingface.co/datasets/amazon_polarity). The goal of this sentiment analysis is to identify whether a review is positive or negative based on the text alone.\n",
    "\n",
    "References:\n",
    "- [Hugging Face: \"Getting Started with Sentiment Analysis using Python\"](https://huggingface.co/blog/sentiment-analysis-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argsort\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/Users/user/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca244052f5e24436ac040aaea529ac13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download dataset from Hugging Face\n",
    "dataset = load_dataset(\"amazon_polarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train'].to_pandas()\n",
    "test = dataset['test'].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (3600000, 3)\n",
      "Test set: (400000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {train.shape}')\n",
    "print(f'Test set: {test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3600000 entries, 0 to 3599999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   label    int64 \n",
      " 1   title    object\n",
      " 2   content  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 82.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Stuning even for the non-gamer</td>\n",
       "      <td>This sound track was beautiful! It paints the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title   \n",
       "0      1                     Stuning even for the non-gamer  \\\n",
       "1      1              The best soundtrack ever to anything.   \n",
       "2      1                                           Amazing!   \n",
       "3      1                               Excellent Soundtrack   \n",
       "4      1  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "\n",
       "                                             content  \n",
       "0  This sound track was beautiful! It paints the ...  \n",
       "1  I'm reading a lot of reviews saying that this ...  \n",
       "2  This soundtrack is my favorite music of all ti...  \n",
       "3  I truly like this soundtrack and I enjoy video...  \n",
       "4  If you've played the game, you know how divine...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Models\n",
    "\n",
    "Let's start with a simple Naive Bayes classifier as the baseline. Because of the size of the dataset, we'll also create smaller sets for faster training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train = train['content']\n",
    "y_train = train['label']\n",
    "\n",
    "X_test = test['content']\n",
    "y_test = test['label']\n",
    "\n",
    "# Smaller datasets for faster training and testing initially\n",
    "X_train_small = X_train[:len(X_train)//20]\n",
    "y_train_small = y_train[:len(X_train)//20]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `TfidfVectorizer` to vectorize the individual words and re-weight the counts based on the inverse-document frequency (penalizing common words that appear frequently such as \"the\", \"a\", \"is\" etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83    200000\n",
      "           1       0.84      0.78      0.81    200000\n",
      "\n",
      "    accuracy                           0.82    400000\n",
      "   macro avg       0.82      0.82      0.82    400000\n",
      "weighted avg       0.82      0.82      0.82    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_pipe = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                    ('classifier', MultinomialNB())])\n",
    "\n",
    "nb_pipe.fit(X_train_small, y_train_small)\n",
    "print(classification_report(y_test, nb_pipe.predict(X_test)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll add a layer of complexity by including bigrams and stopwords in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85    200000\n",
      "           1       0.85      0.84      0.84    200000\n",
      "\n",
      "    accuracy                           0.85    400000\n",
      "   macro avg       0.85      0.85      0.85    400000\n",
      "weighted avg       0.85      0.85      0.85    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'textcat'])\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.union({'ll', 've'})\n",
    "\n",
    "nb_pipe = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                                   stop_words=list(STOP_WORDS))),\n",
    "                    ('classifier', MultinomialNB())])\n",
    "\n",
    "nb_pipe.fit(X_train_small, y_train_small)\n",
    "print(classification_report(y_test, nb_pipe.predict(X_test)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams and stopwords have indeed improved the model performance. Let's try a more complicated machine learning algorithm next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84    200000\n",
      "           1       0.85      0.84      0.84    200000\n",
      "\n",
      "    accuracy                           0.84    400000\n",
      "   macro avg       0.84      0.84      0.84    400000\n",
      "weighted avg       0.84      0.84      0.84    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_pipe = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                                    stop_words=list(STOP_WORDS))),\n",
    "                     ('classifier', SGDClassifier(max_iter=50))])\n",
    "\n",
    "sgd_pipe.fit(X_train_small, y_train_small)\n",
    "print(classification_report(y_test, sgd_pipe.predict(X_test)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SGDClassifier` seems to perform worse than the `MultinomialNB` model. Let's try GridSearch to find more optimal hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87    200000\n",
      "           1       0.87      0.88      0.87    200000\n",
      "\n",
      "    accuracy                           0.87    400000\n",
      "   macro avg       0.87      0.87      0.87    400000\n",
      "weighted avg       0.87      0.87      0.87    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'classifier__alpha': (0.001, 0.0001, 0.00001),\n",
    "          # TODO 'classifier__loss': ('log_loss', 'hinge'), # log_loss = Logistic Regression, hinge = Linear SVM\n",
    "          }\n",
    "\n",
    "sgd_grid = GridSearchCV(sgd_pipe, params, cv=3, verbose=True)\n",
    "sgd_grid.fit(X_train_small, y_train_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87    200000\n",
      "           1       0.87      0.88      0.87    200000\n",
      "\n",
      "    accuracy                           0.87    400000\n",
      "   macro avg       0.87      0.87      0.87    400000\n",
      "weighted avg       0.87      0.87      0.87    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, sgd_grid.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__alpha': 1e-05}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_grid.best_params_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quick GridSearch exercise has further improved the model as expected. Future work will be to increase the range of alphas and compare Logistic Regression versus SVM within SGD, but for now we'll continue with fast exploration iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Models\n",
    "\n",
    "Let's see how pre-trained models from Hugging Face work. We'll try `distilbert-base-uncased-finetuned-sst-2-english` which is one of the most popular text classification models on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example from https://huggingface.co/blog/sentiment-analysis-python\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test data subset for speed\n",
    "X_test_small = X_test[:1000]\n",
    "y_test_small = y_test[:len(X_test_small)]\n",
    "\n",
    "bert = sentiment_pipeline(X_test_small.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.5490386486053467},\n",
       " {'label': 'POSITIVE', 'score': 0.9994763731956482},\n",
       " {'label': 'NEGATIVE', 'score': 0.9965258240699768},\n",
       " {'label': 'NEGATIVE', 'score': 0.9797807931900024},\n",
       " {'label': 'POSITIVE', 'score': 0.9715625643730164}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [1 if key['label'] == 'POSITIVE' else 0 for key in bert]\n",
    "pred[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       498\n",
      "           1       0.89      0.86      0.87       502\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.88      0.88      0.88      1000\n",
      "weighted avg       0.88      0.88      0.88      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_small, pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `distilbert-base-uncased-finetuned-sst-2-english` model appears to perform better than the `SGDClassifier` model, with the tradeoff of it being slower to run. \n",
    "\n",
    "An interesting next step would be to take this pre-trained model and do transfer learning by further training it on the particular dataset here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis\n",
    "\n",
    "The models developed here have a high degree of interpretability. Let's investigate this by looking at the words identified as signaling the most positive and negative sentiments from the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of (word, value) pairs\n",
    "vocab = nb_pipe.get_params()['vectorizer'].vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log probability of features given a class\n",
    "pos = nb_pipe.get_params()['classifier'].feature_log_prob_[1]\n",
    "neg = nb_pipe.get_params()['classifier'].feature_log_prob_[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training the Naive Bayes model calculates probabilities such as $P(\\textrm{heartwarming}\\ |\\ \\textrm{positive}),$ the probability that the word \"heartwarming\" appears in the review text, given that the review is positive. From this we can calculate a **polarity score** for each word:\n",
    "\n",
    "$$\\textrm{polarity}(word) = \\log\\left(\\frac{P(word\\ |\\ \\textrm{positive})}{P(word\\ |\\ \\textrm{negative})}\\right).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Words:\n",
      "timeless\n",
      "pleasantly\n",
      "invaluable\n",
      "downside\n",
      "refreshing\n",
      "heartwarming\n",
      "captures\n",
      "unforgettable\n",
      "frankl\n",
      "wiesel\n",
      "\n",
      "Most Negative Words:\n",
      "worst\n",
      "waste\n",
      "refund\n",
      "poorly\n",
      "redeeming\n",
      "worthless\n",
      "unwatchable\n",
      "uninspiring\n",
      "unreadable\n",
      "disgrace\n"
     ]
    }
   ],
   "source": [
    "# use logarithm quotient rule\n",
    "polarity = pos - neg\n",
    "\n",
    "# indices of the polarity list sorted from least to greatest\n",
    "indices = argsort(polarity)\n",
    "\n",
    "print(\"Most Positive Words:\")\n",
    "for word in vocab:\n",
    "    if vocab[word] in indices[-10:]:\n",
    "        print(word)\n",
    "\n",
    "print(\"\\nMost Negative Words:\")\n",
    "for word in vocab:\n",
    "    if vocab[word] in indices[:10]:\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
